\documentclass[11pt]{article}
% \def\StudentVersion{}

\usepackage{../../common}
\title{Game Playing}
\author{Alexander Rush}
\date{}
\def\LecStr{Alexander Rush}
\def\LecNum{2}
\def\LecTitle{Lecture Notes on Game Playing}
\def\LecDate{}

\begin{document}
\MakeScribeTop{}

\tikzstyle{utri}=[draw, regular polygon, regular polygon sides=3]
\tikzstyle{dtri}=[utri, shape border rotate=180]

\tableofcontents

\section{Board 0}

Show starcraft

\url{https://www.youtube.com/watch?v=LjSXj4cb_Yo}


\section{Board 0.5}

Heuristic examples:


\section{Board 1}

\begin{center}
  \includegraphics[height=13cm]{../pics/game_ais}
\end{center}



\section{Board 2}


Show XKCD
\noindent In this lecture we will use examples from Tic-Tac-Toe, later in the term we will talk about Jeopardy and StarCraft. Backgammon is also a particularly interesting example that we'll discuss briefly. If you're interested MIT has run a great IAP tournaments in the past focusing on Poker and StarCraft. There used to be a substantial reward for solving Go. However, Seven Minutes in Heaven is beyond the scope of this course.

\section{Board 3}

History of Chess in AI
\begin{itemize}
\item 1948 - Alan Turing TuroChamp 
\item 1949-1950 - Claude Shannon
  (http://www.computerhistory.org/chess/full_record.php?iid=stl-430b9bbe92716)
\item 1949 by Claude Shannon 
\item 
\item 1959 - John Mccarthy and students, Allen
  Newell, Cliff Shaw, Herbert Simon (central AI figure)
\item 1973 - Switch to Type-A (brute-force) systems Chess 4.0

\item 1997 - IBM Deep Blue (8-12 depth, 200million moves)

\item 2002 2006 - Commercially available (Deep Fritz) draws with Kramnick
\item Junior - Search for interesting moves.

\item Switch to Go. 
\item Current: Centaur chess. 

\end{itemize}

 

% Game playing is one of the earliest pursuits of AI researchers. John McCarthy's students were already working on chess in 1959. Research into Chess culminated in 1997 when IBM's Deep Blue beat Gary Kasparov, becoming the first computer program to defeat the reining grand champion. Today you can download programs that are world-class at Chess even on standard hardware. 

\begin{itemize}
\item 6.176 - http://mitpokerbots.com/

\item 6.370 - https://www.battlecode.org/

\end{itemize}
\section{Board 4}
Vocabulary: 


\begin{itemize}
\item  instead of minimizing cost, obtain \textbf{utility} 
\item  Utility is only given at \textbf{terminal} states
\item Assume games are \textbf{zero-sum}
\item \textbf{partial information} \textbf{perfect information}, i.e. both players see state of the game. Partial: cards, stratego, starcraft, Perfect
\item \textbf{deterministic} versus \textbf{stochastic}, i.e. both players see state of the game. cards, backgammon, etc. 
\item \textbf{two-player} versus \textbf{multiplayer} chess, checkers versus, settlers, poker

Min/ Max Two players 

\end{itemize}

Chess and go. 

\section{Board 5}


 \air
\begin{center}
\begin{tabularx}{\linewidth}{llX}
  \toprule
  Name (AIMA) & Type & Description \\
  \midrule
\\
 State space & $\mcS$ & All states of the game, e.g. unique board positions \\\\
 Action space & $\mcA$& All actions of the game i.e. moves\\\\
 Actions&  $\msc{Act}: \mcS \mapsto 2^\mcA$ & Actions at state, i.e. available moves. \\\\
 Transition model&  $\msc{Res}:  \mcS \times \mcA \mapsto \mcS $ &  Update the state after a move.  \\\\
 Initial state &  $s_0 \in \mcS$ & The starting state of the game.  \\\\
 \midrule \\
 Player& $\msc{Player}: \mcS \rightarrow \{\mathrm{Min}, \mathrm{Max}\}$ &  The current turn. \\\\
 Term test& $\msc{Term}: \mcS \mapsto \{0, 1\} $ &  Is the state is terminal, i.e. is the game over. \\\\
 Utility & $\msc{Utility}: \mcS \rightarrow \reals$ & The utility  of (terminal) state. \\\\
 \bottomrule
\end{tabularx}
\end{center}

We will be treating game playing as a search problem using a similar abstract specification. However there are several additional elements that we need to include.

Chess:

Settlers of Catan: 

Starcraft

\section{Board 5}

How do we treat a current state in the game? What will Utility be. 


Assumption:  \textbf{Rationality}: Each agent acts simply to gain utility. 


The key challenge of adversarial search is that the agent does not control the decisions of its opponent. Therefore to behave rationally i.e. to minimize or maximize utility, the agent must model its opponents decisions. 

Generally we do this by assuming that the opponent will behave rationally as well, so if we are minimizing utility, we assume the opponent will act to maximize their utility. This leads to the Minimax formula for computing the utility of a nonterminal game state.

\section{Board 6}

\subsection{Game Model}

Under a fully rational game 

\[ \msc{Minimax}(s) = \begin{cases} 
  \censorm{\msc{Utility}(s)} & \mathrm{if\ } \msc{Term}(s)  \\
  \censorm{\max_{a \in \msc{Act}(s)}  \msc{Minimax}(\msc{Res}(s, a))} & \mathrm{if\ } \msc{Player(s)} = \mathrm{Max}  \\
  \censorm{\min_{a \in \msc{Act}(s) } \msc{Minimax}(\msc{Res}(s, a))} & \mathrm{if\ } \msc{Player(s)} = \mathrm{Min} 

\end{cases}\] 

\section{Board 7}

\textbf{Game Tree}

We can expand this function into a data-structure known as a \textbf{game tree}. Each node in this tree represents a move by Max or Min. The triangles pointing up indicate that the node is maximizing over its children, and triangles pointing down indicate that the node is minimizing over its children.

\begin{center}
  \begin{tikzpicture}[sibling distance = 2cm]
    \Tree [ .\node[utri]{}; \edge node[auto=right] {Action 1};  [ .\node[dtri]{};  [  .\node[utri]{}; $\vdots$    ]   [ .\node[utri]{};  $\vdots$ ]
     ]  \edge node[auto=right] {Action 2}; [ .\node[dtri]{};   [ .\node[utri]{}; $\vdots$ ]  [ .\node[utri]{}; $\vdots$ ]  ]  \edge node[auto=left] {Action 3};  [ .\node[dtri]{};   [ .\node[utri]{}; $\vdots$ ]  [ .\node[utri]{}; $\vdots$ ]
    ] ]  
  \end{tikzpicture}
\end{center}

\section{Board 8}

\textbf{Tic-Tac-Toe}
% \subsection{Example: Tic-Tac-Toe}

Let's now consider one of the simplest zero-sum, perfect-information games, Tic-Tac-Toe. Consider the model for the following game state:

Max: O

Min: X

\begin{center}  
\begin{tabular}{c|c|c}
  O  &   &  \\
  \hline
  O  & X & X \\
  \hline
  X  & X & O
\end{tabular}
\end{center}


\section{Board 9}

\begin{exercise}
  What is the model state at this stage of tic-tac-toe?
\end{exercise}
 \air
\begin{center}
\begin{tabularx}{\linewidth}{llX}
  \toprule
  Name & Type & Description \\
  \midrule
\\
 Actions&  \censor{$\msc{Act}(s)= \{$ OTop, OTopRight$\}$} & \censor{There are currently two actions available to play.} \\\\
 Player& \censor{$\msc{Player}(s) = \mathrm{Min}$} & \censor{We will assume that O is minimizing utility and it is her  turn to play.} \\\\
 Term test& \censor{$\msc{Term}(s) = 0 $} & \censor{This is not a terminal state. We have yet to win or draw} \\\\
 Utility & \censor{$\msc{Utility}$} & \censor{The utility function at a terminal state will be 1 for X, -1 for O, 0 for a draw.} \\\\
 \bottomrule
\end{tabularx}
\end{center}

Next we can ask what is the utility that the Min player should assign to this state. We can do this by expanding out the recursive \textsc{Minimax} definition. This gives: 

\section{Board 9}
 
\begin{eqnarray*} 
  \msc{Minimax}(s) = \min \{&& \max \{ \msc{Utility}(\msc{Res}( \msc{Res}(s, \mathrm{OTop})), \mathrm{XTopRight}) \}, \\
 & & \max \{ \msc{Utility}(\msc{Res}( \msc{Res}(s, \mathrm{OTopRight})), \mathrm{XTop}) \} \} 
\end{eqnarray*}

\noindent Since either way O plays, it leads to an X win, this utility of this state is $1$. 

\begin{exercise}
 Can we also write the same calculation as a game tree?
\end{exercise}


\ifthenelse{\isundefined{\StudentVersion}}{
\censor
\begin{center}
  \begin{tikzpicture}[sibling distance = 2cm]
    \Tree [ .\node[dtri]{}; \edge node[auto=right] {OTop}; [  .\node[utri]{}; \edge node[auto=left] {XTopRight}; 1 ] \edge node[auto=left] {OTopRight}; [ .\node[utri]{}; \edge node[auto=left] {XTop}; 1
    ] ]
  \end{tikzpicture}
\end{center}

} {
\censor{}
\vspace{5cm}
} 

\section{Board 10}

Now let's consider a slightly more interesting position from X's perspective

\begin{center}  
\begin{tabular}{c|c|c}
  O  &   &  \\
  \hline
  X  & O &  \\
  \hline
  X  & O & X
\end{tabular}
\end{center}


\begin{exercise}
 Let's write the same calculation as a game tree.
\end{exercise}

\ifthenelse{\isundefined{\StudentVersion}}{
\censor{}

\begin{center}
\scalebox{0.8}{\begin{tikzpicture}[sibling distance = 2cm]
    \Tree [ .\node[utri]{}; \edge node[auto=right] {XTop};  [ .\node[dtri]{}; \edge node[auto=right, yshift=0.5cm] {OTopRight}; [  .\node[utri]{}; \edge node[auto=left] {XRight}; 0 ] \edge node[auto=left] {ORight}; [ .\node[utri]{}; \edge node[auto=left] {XTopRight}; 0
    ] ] \edge node[auto=right] {XTopRight}; [ .\node[dtri]{}; \edge node[auto=right] {OTop};  -1 \edge node[auto=left] {ORight}; [ .\node[utri]{}; \edge node[auto=left] {XTop}; 0 ] 
     ]  \edge node[auto=left] {XRight}; [ .\node[dtri]{}; \edge node[auto=right] {OTop};  -1 \edge node[auto=left] {OTopRight}; [ .\node[utri]{}; \edge node[auto=left] {XTop}; 0 ] 
    ] ]  
  \end{tikzpicture}}
\end{center}
} {

\censor{}
\vspace{10cm}
}

The max player (X) can at best play for a draw here. 
Clearly the right play is XTop which gives the utility of 0. 

Note though that for these problems we are only considering a branching factor of at most 3, and a depth of 2. In practice the tree grows very large, very quickly.
AIMA has a nice graphic showing a partial  expansion of a game tree for tic-tac-toe from scratch. 

\begin{center}
  \includegraphics{../pics/tictactoe}
\end{center}

\section{Board 11}
% \section{Search for Game Playing}

% As with standard search, we have to decide how to 
% enumerate possible game states in order to find a path to take. We do this by implementing search over the Minimax game tree. 

- Graph Search for games 

- Depth-first search

- Tree Search

In particular, we utilize uninformed depth-first tree search where are terminal states are goal states. 
\begin{exercise}
  Why do we not need to use graph search for games? Why not use uniform-cost search?
\end{exercise}

- DFS with tree search has great memory efficiency.

In this situation, DFS has clear advantages over BFS in terms of
memory-efficiency. Because of the nature of game playing, we do not
have to worry about maintaining the costly expand list.  


% The minimax algorithm can be seen as a special case of DFS. For a game with $m$ levels to the furthest terminal state it requires $O(b^m)$ steps to compute this function. Of course this is   
\section{Board 12}
Note also
that instead of explicitly using a stack, we can implement DFS using
recursion. Each recursive call to \textsc{Minimax} simulates a pushing
and popping from the stack. This leads to a very simple implementation
of Minimax Search.  \air

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \Procedure{Minimax}{$s$}

  \If{$\msc{Term}(s)$}
  \Return{$\msc{Utility}(s)$}
  \ElsIf{$\msc{Player}(s) = \mathrm{Max}$}
  \State{$v \gets \censorm{-\infty}$}
  \For{$a \in \msc{Act}(s)$}
  \State{\censor{$v \gets \max\{v, \msc{Minimax}(\msc{Res}(s, a))\}$}}
  \EndFor{}
  \ElsIf{$\msc{Player}(s) = \mathrm{Min}$}
  \State{$v \gets \censorm{\infty}$}
  \For{$a \in \msc{Act}(s)$}
  \State{\censor{$v \gets \min\{v, \msc{Minimax}(\msc{Res}(s, a))\}$}}
  \EndFor{}
  \EndIf{}
  \State{\Return{$v$}}
  \EndProcedure{}
\end{algorithmic}
\end{algorithm}


% \noindent The pseudo-code here is basically a direct implementation of the mathematical definition of \textsc{Minimax}. However, the recursive implementation means we search deeper before searching broader.

\section{Board 13}

\begin{exercise}
  What is the space and time complexity of this algorithm?
\end{exercise}

\censor{
  Again let $m$ be the depth of the deepest terminal state and $b$ be
  the max branching factor. This algorithm need to enumerate each
  possible state to this point giving a run-time of $O(b^m)$. However
  at any one time, it only needs memory for all actions at each state
  along the path to the deepest goal. This gives a memory complexity
  of $O(bm)$.}


\section{Board 14}

\subsection{Alpha-Beta Pruning}

We can further speed up game-playing search by exploiting some properties about the game tree.  One nice property of the $\max$ ($\min$) operator, is that if we know after seeing $i$ elements of the set that known of the future elements will be greater (less) then it is not necessary to compute these values at all. We can therefore \textbf{short circuit} the computation and return.

  \begin{tikzpicture}[sibling distance = 2cm]
    \Tree [ .\node[utri]{};   
    [ .\node[dtri]{};  [  .\node[utri]{}; 5    ]   [ .\node[utri]{};  10 ] ]  
    [ .\node[dtri]{};   [ .\node[utri]{}; 3 ]  [ .\node[utri]{}; 12 ]     [ .\node[utri]{}; -1 ]  [ .\node[utri]{}; 1 ]
    ] ]  
  \end{tikzpicture}

\section{Board 15}
  \begin{exercise}
    If instead of $-1$ the utility was $-1000$, would anything change? What if instead $5$ was equal to $2$?
  \end{exercise}


% Consider the following example:

% \vspace{1cm}



%   \begin{exercise}
%     Go through this example and calculate the value of each state of the game tree top-down, left-to-right.
%     What is the minimax value of the top state?
%   \end{exercise}
% \censor{}

% \censor{}

What happened in this case was that the already computed value $5$ already dominated the value of $3$ in the min. Since $3$ is already lower than $5$ there is no need to keep on calculating the rest of the values. 

\section{Board 15}

\textbf{Alpha-beta pruning} speeds up minimax search while still given the exact result.. 



\[ \max\{ \alpha, \ldots,  \min\{\ldots, v, \ldots, v' \} \}\]
  
\noindent Computed the value of $\alpha$ and $v$, do we need to compute the rest of the min including $v'$. Consider the two cases:

\begin{enumerate}
\item  $v > \alpha$; inner $\min$ may end with a value higher than $\alpha$ 
\item  $v \leq \alpha$; inner $\min$ cannot change outer max,  value of $v'$ is uninformative. 
\end{enumerate}

Examples the \textbf{alpha} value was $\alpha = 5$ and $v=3$. 

\textbf{Generalize}:

\[ \max\{ \ldots, \alpha, \ldots, \min\{ \max\{\ldots \min\{\ldots, v, \ldots, v' \} \}\} , \ldots\}\]

If there is a future value $v'$ with $v' \leq v$ it may be selected by the inner $\min$. However we know $v' < v \leq \alpha$.

so it will not be selected by the outer $\max$. The bigger $\alpha$ we have, the more likely we can apply this pruning, so we maintain the largest $\alpha$ seen at ancestor $\max$ node. 

% We can also reverse this trick for $\min$ nodes by maintaining the smallest $\beta$ value:

\section{Board 16}

\textbf{Reverse trick}

\[ \censorm{\min\{ \ldots, \beta, \ldots, \max\{ \min\{\ldots \min\{\ldots, v, \ldots, v' \} \}\} , \ldots\} }\]


\section{Board 17}
The modified Minimax search algorithm is shown below.
The upside of this method is that it yields a significant speed-up in practice with very minimal bookkeeping and no effect on the result. A rare combination.

\begin{algorithm}
\begin{algorithmic}[1]
  \Require{$\alpha$ is highest value of ancestor max-node, 
    $\beta$ is lowest value of ancestor min-node}
  \Procedure{AlphaBetaMinimax}{$s$, $\alpha$, $\beta$}

  \If{$\msc{Term}(s) = 1$}
  \Return{$\msc{Utility}(s)$}
  \ElsIf{$\msc{Player}(s) = $ Max}
  \State{$v \gets -\infty$}
  \State{$v \gets $\censor{}}
  \For{$a \in \msc{Act}(s)$}
  \State{$v \gets$ \censorm{$\max\{v, \msc{AlphaBetaMinimax}(\msc{Res}(s, a), \alpha, \beta)\}$}}
  \If{$v \geq \beta$}
  \Return{$v$}
  \EndIf{}
  \State{$\alpha \gets \max\{\alpha, v\}$}
  \EndFor{}
  \ElsIf{$\msc{Player}(s) = $Min}
  \State{$v \gets \infty$}
  \For{$a \in \msc{Act}(s)$}
  \State{$v \gets$ \censorm{$\min\{v, \msc{AlphaBetaMinimax}(\msc{Res}(s, a), \alpha, \beta)\}$}}
  
  \If{\censorm{$v \leq \alpha$}}
  \Return{$v$}
  \EndIf{}
  \State{\censorm{$\beta \gets \min\{\beta, v\}$}}
  \EndFor{}
  \EndIf{}
  \State{\Return{$v$}}
  \EndProcedure{}
\end{algorithmic}
\end{algorithm}

Doesn't change complexity in the worst case, practically gives a 2x speed up. 

ordering moves matters now!

\section{Board 18}

\textbf{Chess} 

Average branching factor: 35

Average depth: 40

(apparently there was a 277 move game once)


$O(35^{40})$ is cartoonishly large (actually worse!) 

\[ 57905761067641178540192733082440108773880638182163238525390625\] 

\section{Board 19}

Depth-limited scoring function

Heuristic $h$

\[ \msc{H-Minimax}(s, d) = \begin{cases} 
  \msc{Utility}(s) & \mathrm{if\ } \msc{Term}(s)  \\
  h(s) & \mathrm{if\ } d > \mathrm{limit}   \\
  \max_{a \in \msc{Act}(s) } \msc{H-Minimax}(\msc{Res}(s, a), d+1) & \mathrm{if\ } \msc{Player(s)} = \mathrm{Max}  \\
  \min_{a \in \msc{Act}(s) } \msc{H-Minimax}(\msc{Res}(s, a), d+1) & \mathrm{if\ } \msc{Player(s)} = \mathrm{Min} \end{cases}\] 

No notion of admissable or consistent here. 

$h: \mcS \mapsto \reals$

\section{Board 20}

Heuristics
Perhaps the simplest heuristic is the value system taught to beginners learning to play chess. This system simply assigns value to the \textbf{material remaining} on the board. 
\air

\centerline{\includegraphics[width=8cm]{../pics/chessvalues1}}
\air

\section{Board 21}

\subsection{Heuristics}

Generally game playing heuristics estimate how good a position is for each player. Note that unlike search heuristics we will not make any attempt to maintain admissibility or consistency here, although similar conditions do exist.


However, note that because this is just an approximation and ignores so much of the other complications of chess, there have been many, many other heuristics for this problem. (Wikipedia is amazing.) I am not a chess player, but it is pretty remarkable to me that not even the relative values of bishop and knight are consistent.

\centerline{ \includegraphics[width=16cm]{../pics/chess_value}}
\vspace{1cm}

Another method that is used is brute force calculation of certain positions. In chess this often means utilizing an end-game solver to play out the results of a large set of standard positions (such as a kings and pawns ending) . Given that each extra depth is a multiplicative cost, having an endgame database can turn a failed search into a successful one.


- LandmarkLowerBound(S,G) — StarCraft’s tech tree
imposes many prerequisites on actions. These actions are
known in the search literature as landmarks. Given this sequence
of non-concurrent landmark actions, we sum the individual
durations of actions not yet created to form an admissible
lower bound for our search.

- ResourceGoalBound(S,G) — Summing the total consumed
resource cost of units in a goal gives us a lower bound
on the resources required to construct the goal optimally.
Performing a quick search to determine the makespan of
producing only these resources is an admissible heuristic.


Monte-carlo style- simulate out the remaining game

Based on playouts. 

\section{Board 22}

\subsection{Expecti-Max}


So far we have consider only deterministic games with perfect information. We will end by discussing games like Backgammon, which have perfect information but incorporates randomness. Each turn in backgammon consists of two steps:

\begin{itemize}
\item Roll two dies
\item Select checkers to move based on dice values. 
\end{itemize}


\section{Board 23}
Our focus will be on the question of how to model our opponent Min's action. Since we don't know what value they will roll, we could be pessimistic and assume that they get the best possible roll ($\min$ over rolls) or optimistic assume they get the worst possible roll ($\max$ over rolls). 


Instead, we will directly model the randomness of the roll. We treat the dice roll as a third play Exp and the value of the dice as an action $a$. Given the state of the world $s$, we say the probability of Exp taking action $a$ is given as $P(a | s)$. 

In the case of Backgammon, there are $36$ possible actions corresponding to each pair of dice rolls, each with probability $\frac{1}{36}$. The turn order then alternates: Exp, Min, Exp, Max, Exp, Min, ...
Once we explicitly incorporate the last roll into the state $s$, the moves of Max and Min become deterministic.  


\section{Board 23}

\[\mathbb{E}_{x|y}[f(x)] = \sum_{x} p(x|y)f(x) \]

In the case of the utility of a state, this gives us a method for computing the expected utility of an Exp state.

% In other games the randomness may take various other forms. 


% consider the case of a game with randomness. We model randomness as a third player Exp. Assume that at random states there is a probability distribution $p(a | s)$ of selecting an action $a$. Define expectation of 

\[ \mathbb{E}_{a|s}[\msc{Utility}(\msc{Res}(s, a))]= \sum_{a \in \msc{Act}(s) } p(a | s) \msc{Utility}(\msc{Res}(s, a))  \]  

This formula can then be directly plugged into minimax, to yield an algorithm known as expectimax

\section{Board 24}

\[ \msc{E-Minimax}(s) = \begin{cases} 
  \censorm{\msc{Utility}(s)} & \mathrm{if\ } \msc{Term}(s)  \\
  \censorm{\max_{a \in \msc{Act}(s) } \msc{E-Minimax}(\msc{Res}(s, a))} & \mathrm{if\ } \msc{Player(s)} = \mathrm{Max}  \\
  \censorm{\min_{a \in \msc{Act}(s) } \msc{E-Minimax}(\msc{Res}(s, a))} & \mathrm{if\ } \msc{Player(s)} = \mathrm{Min} \\ 
  \censorm{\sum_{a \in \msc{Act}(s) } p(a | s) \msc{E-Minimax}(\msc{Res}(s, a))} & \mathrm{if\ } \msc{Player(s)} = \mathrm{Exp} \end{cases}\] 


\end{document}